---
title: "Half-Length Rewrites"
date: "2026-02-07"
tags: [nulla-dies]
---

# Half-Length Rewrites

Today's writing exercise: take a verbose paragraph and rewrite it in half the words or fewer, preserving all claims.

## 1. Renormalization Group (97 → 31 words)

> The key insight behind renormalization group methods is essentially that when you are trying to understand the behavior of a physical system at large scales, it turns out that you don't actually need to keep track of all of the microscopic details. What you can do instead is to systematically integrate out the short-distance degrees of freedom, and in doing so you obtain an effective description that captures all of the relevant physics at the scale you care about. It is perhaps worth noting that this procedure also reveals which quantities are universal and which are not.

Renormalization methods permit systematically ignoring dynamics below a specified scale.  The result is an effective theory for physics above that scale.  We call 'universal' quantities which approach a definite value for very large scales.

## 2. Causal Inference (89 → 32 words)

> A fundamental difficulty with causal inference is that we can never directly observe counterfactual outcomes. When we say that a treatment caused an effect, what we really mean is that the outcome that was observed under treatment differs from the outcome that would have been observed had the treatment not been administered, but of course we cannot observe both of these outcomes for the same individual. This is sometimes referred to as the "fundamental problem of causal inference," and it means that causal claims always require some form of untestable assumptions.

The 'fundamental problem of causal inference' is that causal effects are measured against counterfactual outcomes.  We cannot observe counterfactuals, so we must make untestable assertions of them in order to proceed.

## 3. Fisher Information (103 → 49 words)

> There is a deep connection between the geometry of a parameter space and the statistical properties of the models that live on it. The Fisher information matrix defines a Riemannian metric on the space of probability distributions, and this metric has the remarkable property that it is the unique metric (up to a constant factor) that is invariant under sufficient statistics. What this means in practice is that distances in parameter space, when measured using the Fisher metric, correspond to how statistically distinguishable two nearby models are, regardless of how we happen to have parameterized them.

Model-space is a Riemannian space with the Fisher information matrix as its metric.  The Fisher information is the unique metric which is invariant under reparametrizations of the underlying model.  The induced Fisher-Rao distance therefore furnishes a parametrization-independent measure of statistical distance between two models of the family in question.

*Written with Claude.*
