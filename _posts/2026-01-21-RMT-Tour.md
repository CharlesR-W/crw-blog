---
title: "A Whirlwind Tour of Random Matrix Theory"
date: 2026-01-21
motivation: "RMT looks exactly like statistical mechanics—partition functions, Boltzmann weights, saddle points, diagrammatics. Is that coincidence or structure?"
background: "Stat mech intuition (know what a partition function is), basic linear algebra. The log-gas perspective makes the whole subject click: eigenvalues are particles with logarithmic repulsion."
llm: "Claude"
tags: [seed]
math: true
---

# A Whirlwind Tour of Random Matrix Theory

## Why RMT Looks Like Statistical Mechanics

Here's the punchline up front: random matrix theory *is* statistical mechanics, just for eigenvalues instead of particles. The partition function, the Boltzmann weights, the saddle-point approximations, the diagrammatics—it's all there, wearing a slightly different hat.

Consider a random $N \times N$ Hermitian matrix $H$ drawn from the Gaussian Unitary Ensemble (GUE):

$$
P(H) \propto \exp\left( -\frac{N}{2} \text{Tr}(H^2) \right)
$$

This is a Boltzmann weight! The "energy" is $E(H) = \frac{N}{2}\text{Tr}(H^2)$, and we're at inverse temperature $\beta = 1$.

Now here's the magic. The eigenvalues $\{\lambda_1, \ldots, \lambda_N\}$ have a joint distribution:

$$
\boxed{P(\lambda_1, \ldots, \lambda_N) \propto \prod_{i < j} |\lambda_i - \lambda_j|^2 \cdot \prod_i e^{-\frac{N}{2}\lambda_i^2}}
$$

That $|\lambda_i - \lambda_j|^2$ factor is the **Vandermonde determinant** squared. It's a repulsive interaction between eigenvalues—they don't want to be near each other. This is eigenvalue repulsion, and it's what makes RMT nontrivial.

Rewrite this in Boltzmann form:

$$
P(\boldsymbol{\lambda}) \propto \exp\left( -N \sum_i \frac{\lambda_i^2}{2} + 2\sum_{i < j} \log|\lambda_i - \lambda_j| \right)
$$

This is a **log-gas**: particles on a line with quadratic confining potential and logarithmic repulsion. The $N$ in front is like inverse temperature—as $N \to \infty$, we're in the "cold" limit where fluctuations freeze out and we get a deterministic density.

## The Semicircle Law: Saddle Point of the Log-Gas

In the large-$N$ limit, we can replace the sum over eigenvalues with an integral over a density $\rho(\lambda)$:

$$
-\frac{1}{N^2} \log P \approx \int d\lambda \, \frac{\lambda^2}{2} \rho(\lambda) - \int d\lambda \, d\mu \, \rho(\lambda) \rho(\mu) \log|\lambda - \mu|
$$

subject to $\int \rho = 1$.

Varying with respect to $\rho$ (with Lagrange multiplier for normalization):

$$
\frac{\lambda^2}{2} = \int d\mu \, \rho(\mu) \log|\lambda - \mu| + \text{const}
$$

Differentiate both sides:

$$
\lambda = \text{P.V.} \int \frac{\rho(\mu)}{\lambda - \mu} d\mu
$$

This is a singular integral equation. The solution is the **Wigner semicircle**:

$$
\boxed{\rho(\lambda) = \frac{1}{2\pi} \sqrt{4 - \lambda^2}, \quad |\lambda| \leq 2}
$$

The eigenvalues pack into a semicircular density, supported on $[-2, 2]$.

## Diagrammatics: Where the Feynman Rules Come From

The perturbative expansion of matrix integrals gives rise to **ribbon graphs** (fat graphs, maps). Here's why.

Consider the integral:

$$
\langle \text{Tr}(H^{2k}) \rangle = \int \mathcal{D}H \, e^{-\frac{N}{2}\text{Tr} H^2} \text{Tr}(H^{2k})
$$

Expand in Wick contractions. The propagator is:

$$
\langle H_{ij} H_{kl} \rangle = \frac{1}{N} \delta_{il} \delta_{jk}
$$

Each contraction connects index pairs. Because we're tracing, indices form loops. Each loop gives a factor of $N$ (from $\delta_{ii} = N$).

The combinatorics of which contractions are possible, and how many index loops result, is exactly the combinatorics of **surfaces**:

- **Planar diagrams** (genus 0): $O(N^2)$
- **Torus diagrams** (genus 1): $O(N^0)$
- **Higher genus**: $O(N^{2-2g})$

This is the **$1/N$ expansion** = **topological expansion**. The large-$N$ limit picks out planar diagrams, which is why it's solvable.

The Feynman rules are:
- Propagator: $\langle H_{ij} H_{kl} \rangle = \frac{1}{N} \delta_{il}\delta_{jk}$ (draw as double line / ribbon)
- Vertex: $\text{Tr}(H^k)$ (k ribbons meeting at a point, cyclically ordered)
- Each closed index loop: factor of $N$

**This is the origin of free probability:** at large $N$, only planar diagrams survive. The algebraic structure that captures "only planar diagrams contribute" is exactly freeness. Non-crossing partitions in free probability = planar graphs in the $1/N$ expansion.

## Universality: Why the Details Don't Matter

Here's the remarkable thing: the local statistics of eigenvalues are **universal**. Near the bulk of the spectrum, the correlations between eigenvalues look the same whether you started with Gaussian, uniform, or whatever—as long as you have the same symmetry class.

### The Three Classical Ensembles

- **GUE** (Gaussian Unitary Ensemble): Complex Hermitian matrices, $\beta = 2$
- **GOE** (Gaussian Orthogonal Ensemble): Real symmetric matrices, $\beta = 1$
- **GSE** (Gaussian Symplectic Ensemble): Quaternionic self-dual matrices, $\beta = 4$

The $\beta$ parameter controls the strength of eigenvalue repulsion:

$$
P(\boldsymbol{\lambda}) \propto \prod_{i<j} |\lambda_i - \lambda_j|^\beta \cdot \prod_i e^{-\frac{\beta N}{4} \lambda_i^2}
$$

**Theorem (Dyson):** The Gaussian ensembles are the *unique* unitarily/orthogonally/symplectically invariant ensembles with independent matrix entries.

This is why Gaussian is special: symmetry + independence = Gaussian. Any other invariant ensemble must have correlated entries.

### Universality Classes

**Bulk universality:** In the interior of the spectrum, eigenvalue correlations are described by the **sine kernel**:

$$
K(\lambda, \mu) = \frac{\sin(\pi \rho_0 (\lambda - \mu))}{\pi(\lambda - \mu)}
$$

where $\rho_0$ is the local density. This holds for *any* Wigner matrix (iid entries with finite moments).

**Edge universality:** At the edge of the spectrum, fluctuations of the largest eigenvalue follow the **Tracy-Widom distribution**:

$$
P(\lambda_{\max} \leq 2 + sN^{-2/3}) \to F_\beta(s)
$$

where $F_\beta$ is the Tracy-Widom distribution for $\beta = 1, 2, 4$.

### RG Perspective on Universality

Why universality? Think renormalization group.

The "microscopic" theory is specified by the matrix distribution $P(H)$. As we coarse-grain (integrate out high-energy modes / look at longer wavelength correlations), we flow to a fixed point.

For invariant ensembles, the flow is governed by:
1. **Symmetry class** (determines $\beta$)
2. **Confining potential** (determines bulk density)
3. **Nothing else** at long wavelengths

The irrelevant operators (higher moments, fine structure of the distribution) get washed out. Only the "relevant" information—symmetry and mean density—survives.

This is exactly like the central limit theorem being an RG fixed point for sums of random variables, or like critical phenomena having universal exponents.

## Edge Fluctuations and Tracy-Widom

The Tracy-Widom distributions $F_\beta$ describe the fluctuations of the largest eigenvalue. They're remarkable: they appear everywhere—longest increasing subsequences, growth processes, KPZ universality class, etc.

For GUE ($\beta = 2$), Tracy-Widom is defined via a Painlevé equation. The PDF looks roughly like:

- Left tail: $f(s) \sim e^{-|s|^3/12}$ (superexponential decay)
- Right tail: $f(s) \sim e^{-\frac{2}{3}s^{3/2}}$ (stretched exponential)

The asymmetry reflects that eigenvalues can fluctuate inward easily (other eigenvalues push back less) but outward is suppressed by the potential.

## Connection to Free Probability

As $N \to \infty$, random matrices become **free random variables** in Voiculescu's sense.

Two random matrices $A$ and $B$ (from independent ensembles) are "free" in the large-$N$ limit. Their joint distribution is determined by their individual spectral distributions plus the freeness relation.

The key tool: instead of characteristic function $\mathbb{E}[e^{itX}]$, use the **Cauchy transform**:

$$
G_X(z) = \mathbb{E}\left[\frac{1}{z - X}\right]
$$

For free random variables, there's an additive convolution formula involving the **R-transform**:

$$
R_{X+Y}(z) = R_X(z) + R_Y(z)
$$

And a multiplicative one involving the **S-transform**:

$$
S_{XY}(z) = S_X(z) \cdot S_Y(z)
$$

The semicircle law is the free analog of the Gaussian. Marcenko-Pastur is the spectral distribution of $\frac{1}{N} X X^T$ where $X$ is $N \times M$ with iid entries, as $N, M \to \infty$ with $M/N \to \gamma$.

Free probability = non-commutative probability = large-$N$ limit of random matrices.

## What We Didn't Cover

- **Determinantal point processes** and their role in exact formulas
- **Orthogonal polynomial methods** (the workhorse for exact computations)
- **$\beta$-ensembles** for general $\beta$ (not just 1, 2, 4)
- **Sparse matrices** and their different universality classes
- **Products of random matrices** and Lyapunov exponents
- **Non-Hermitian random matrices** (Ginibre ensemble, circular law)
- **Connections to number theory** (Montgomery-Odlyzko, zeros of zeta)

For the comprehensive treatment, see Mehta's book or the lecture notes by various Italian mathematicians (you know the ones).

---

*See also: [Free Probability], [Extreme Value Theory].*
