<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Simple Exercises on Transformers 1: Self-Attention</title>

  <!-- Tag styling -->
  <style>
    .tags { margin: 0.5em 0 1.5em 0; }
    .tag {
      display: inline-block;
      padding: 2px 8px;
      margin: 2px;
      background: rgba(255,255,255,0.08);
      border-radius: 3px;
      font-size: 0.85em;
      text-decoration: none;
      color: var(--muted);
    }
    .tag:hover { background: rgba(255,255,255,0.15); color: var(--accent); }
  </style>

  <!-- MathJax configuration for LaTeX rendering -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        ignoreHtmlClass: "tex2jax_ignore",
        processHtmlClass: "tex2jax_process"
      },
      svg: {
        fontCache: 'global'
      },
      startup: {
        pageReady: function () {
          return MathJax.startup.defaultPageReady().then(function () {
            console.log('MathJax is loaded and ready');
          });
        }
      }
    };
  </script>
  <script type="text/javascript" async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-svg.js"></script>

  <!-- Site stylesheet -->
  <link rel="stylesheet" href="/crw-blog/assets/css/style.css">
</head>

<body>
  <!-- Header and site title -->
  <header>
    <h1>Fruit of Preterition</h1>
    <nav>
      <ul>
        <li><a href="/crw-blog/">Home</a></li>
        <li><a href="/crw-blog/seeds/">Seeds <span class="wip-badge">WIP</span></a></li>
        <li><a href="/crw-blog/tags/">Tags <span class="wip-badge">WIP</span></a></li>
        <li><a href="/crw-blog/reading/">Reading <span class="wip-badge">WIP</span></a></li>
        <li><a href="/crw-blog/nulla-die/">Nulla Die</a></li>
      </ul>
    </nav>
  </header>

  <!-- Main content of the page -->
  <main>
    <div class="site-container">
      <div class="content">
        
        <div class="tags">
          
          <a href="/crw-blog/tags/#ml" class="tag">ml</a>
          
        </div>
        
        <p>I’ve heard a few times from friends that they’re repeatedly surprised to discover they don’t actually understand how transformers work as well as they had thought.  Here’s a simple exercise I did for myself that I think helped a lot (though with the obvious caveat that I am presumably subject to a similar bias to overestimate my understanding ;) )</p>

<p>To do this, you should have an understanding of the transformer architecture - there are millions of tutorials that teach this and you’ll profit by studying them - if you want to spend some time reflecting on it, the famous “<a href="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framewok for Transformer Circuits</a>” is very good.  Also, the transformers circuit thread also includes <a href="https://transformer-circuits.pub/2021/exercises/index.html">these exercises</a>.  I haven’t done more than glanced at them, but they’re doubtless quite good.</p>

<p>The final goal of these exercises is to design your on induction head.  An induction head is one that does “AB…A—&gt;B”.  <strong>We’ll be using two self-attention layers with no MLPs</strong>.  The simplification that makes this fun is that you’re allowed to <strong>completely abstract away the residual stream</strong> : you are allowed to write and query arbitrary semantic information about a token into its residual stream, respecting locality and causality.</p>

<p><img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/kcomp_diagram.png" alt="Credit to ARENA 1.2" style="max-width:100%; height:auto;" />
This image from <a href="https://arena-chapter1-transformer-interp.streamlit.app/[1.2]_Intro_to_Mech_Interp">ARENA 1.2</a> “Introduction to Mechanistic Interpretability” explains it perfectly; it depicts an induction process which does “AB….A –&gt; predict B”.</p>

<p><strong>Your job is to write out the Q, K, V, and O matrices!</strong>
(also take a look at the consequent QK and OV products)</p>

<p><strong>Exercises:</strong> Try doing each of these <strong>WITHOUT and then WITH</strong> a positional encoding, to get a feel for exactly how they work and don’t.  You should assume for all that we use a causal mask (after maybe trying one or two without).</p>
<ol>
  <li>Write a conditional “IF current token is A, then write B” (“A –&gt; B”)</li>
  <li>Then try “If previous token is A, write B” (“AX –&gt; B”,  where it should predict this no matter what single token X is)</li>
  <li>Then try for “if previously ever saw A, then predict B” (“A…X–&gt;B”)</li>
  <li>Try “if previous 2 tokens were AB, write C” (“AB –&gt; C”)</li>
  <li>Try the ‘cycle’ which does “if current token is A then write B, if it’s B then write C, …., and if current token is N, write A.” (4 tokens should be enough : A –&gt; B –&gt; C –&gt; D –&gt; A)</li>
  <li>Lastly, do the induction head.  An induction head implements “If current token is A AND previously saw AB THEN write B” (“AB…A–&gt;B”).</li>
</ol>

<p><strong>NB:</strong> It may be hard to get rid of some ‘interference terms’ in the ‘cycle’ exercise, which you should note as qualitatively important, but it doesn’t seem super useful /easy to manually try and design around them.  I played with it for 20 minutes and couldn’t figure it out, iirc.</p>

<hr />
<h3 id="gpt5s-work-for-exercise-1-yes-i-checked-it-and-edited-it-dont-worry-i-dont-trust-it-either">GPT5’s Work for Exercise 1 (yes I checked it and edited it; don’t worry I don’t trust it either)</h3>
<p>(some typographical errors with the LaTeX here that I couldn’t iron out - nothing detracting from meaning.  ‘Live Laugh Love LaTeX’  right)</p>
<h3 id="worked-example--designing-a--b">Worked Example — Designing “A → B”</h3>

<p>We implement: <strong>If the current token is A, then write B</strong> (“A → B”). Single attention head, single self-attention layer, no MLP. For simplicity, we treat $W_O W_V$ as one combined map $W_{OV}$.</p>

<p>The residual stream is a short <strong>semantic tape</strong> of flags/symbols read/written “all at once.” You could realize this as a one-hot basis; with more context you might store a list such as [“current_token is A”, “prev_token is X”, …] (not needed here). The downside vs. Turing machines is linear, parallel access rather than a movable head.</p>

<hr />

<h3 id="1-semantic-encoding">1. Semantic encoding</h3>

<p>We view encode(token) as a small set (or linear combo) of semantic flags:</p>

<ul>
  <li>encode(A) → [“current_token is A”]</li>
  <li>encode(B) → [“current_token is B”]</li>
  <li>encode(X) → [“current_token is X”]</li>
</ul>

<hr />

<h3 id="2-weights-w_q-w_k-w_ov-semantic-maps">2. Weights $W_Q$, $W_K$, $W_{OV}$ (semantic maps)</h3>

<p>Goal: fire <strong>“is current_token A?”</strong> in $Q$, then $W_{OV}$ writes <strong>“predicted_token is B”</strong>.</p>

<h4 id="query-vector-and-weight-matrix">Query vector and weight matrix</h4>

<p>We define the query vector for a given token as:</p>

\[\vec{q}_{\mathrm{current\_token}} = \mathrm{encode}(\mathrm{current\_token}) \cdot W_Q\]

<p>Choose $W_Q$ so only the query for “is current_token A?” can fire:</p>

\[W_Q =
\begin{bmatrix}
\text{"Q: is current\_token A?"} &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots
\end{bmatrix}\]

<ul>
  <li>If current_token = A:<br />
$\vec{q}_{\mathrm{current_token}} = \big[ \mathrm{Q:~is~current_token~A?} \big]$ (fires strongly).</li>
  <li>Else: $\vec{q}_{\mathrm{current_token}} = \vec{0}$.</li>
</ul>

<h4 id="key-vector-and-weight-matrix">Key vector and weight matrix</h4>

<p>Similarly,</p>

\[\vec{k}_{\mathrm{current\_token}} = \mathrm{encode}(\mathrm{current\_token}) \cdot W_K\]

<p>Pick $W_K$ so the key for each token identity sits in a matching position to the query:</p>

\[W_K =
\begin{bmatrix}
\text{"K: current\_token is A"} \\
\text{"K: current\_token is B"} \\
\text{"K: current\_token is X"} \\
\vdots 
\end{bmatrix}\]

<ul>
  <li>If current_token = A:<br />
$\vec{k}_{\mathrm{current_token}} = \big[ \mathrm{K:~current_token~is~A} \big]$</li>
  <li>If current_token = B:<br />
$\vec{k}_{\mathrm{current_token}} = \big[ \mathrm{K:~current_token~is~B} \big]$</li>
  <li>If current_token = X:<br />
$\vec{k}_{\mathrm{current_token}} = \big[ \mathrm{K:~current_token~is~X} \big]$</li>
</ul>

<hr />

<h4 id="w_ov--combined-outputvalue-map">$W_{OV}$ — combined output–value map</h4>

\[\begin{array}{c|ccc}
 &amp; \text{"QK: current\_token is A"} &amp; \text{"QK: current\_token is B"} &amp; \text{"QK: current\_token is X"} \\
\hline
\text{"OV: write predicted\_token is B"} &amp; 1 &amp; 0 &amp; 0 \\
\text{"OV: write seen A recently"} &amp; 0.8 &amp; 0 &amp; 0 \\
\text{noop} &amp; 0 &amp; 0 &amp; 0
\end{array}\]

<p>Interpretation:</p>
<ul>
  <li>If attention locks onto <strong>current_token is A</strong>, then write <strong>predicted_token is B</strong> and <strong>seen A recently</strong>.</li>
  <li>Otherwise, only NOOP.<br />
Some rows store intermediate computations for later use.</li>
</ul>

<hr />

<h3 id="3-attention-computation-semantic-view">3. Attention computation (semantic view)</h3>

<p>(note we only consider a ‘one-entry’ attention matrix - we’ll pretend for simplicity that all the other attention goes to the BOS token and does nothing)
Easy as cake:</p>

\[\mathrm{score} = \vec{q}_{\mathrm{current\_token}} \cdot \vec{k}_{\mathrm{current\_token}}\]

<ul>
  <li>If current_token = A: dot product large → attention weight $\approx 1$ on self.</li>
  <li>Else: dot product = 0 → weight $\approx 0$.</li>
</ul>

<hr />

<h3 id="4-end-to-end-semantics">4. End-to-end semantics</h3>

<p>When current_token = A:</p>
<ol>
  <li>$\vec{q}_{\mathrm{current_token}}$ = “Q: is current_token A?”</li>
  <li>$\vec{k}_{\mathrm{current_token}}$ = “K: current_token is A” → strong match</li>
  <li>$W_{OV}$ fires rules:
    <ul>
      <li>write predicted_token is B</li>
      <li>write seen A recently</li>
    </ul>
  </li>
</ol>

<p>Compactly:</p>

\[\mathrm{Output(current\_token = A)} \approx W_{OV}
= \begin{bmatrix}
\mathrm{predicted\_token~is~B} \\
\mathrm{seen~A~recently} \\
\mathrm{noop}
\end{bmatrix}
\propto \mathrm{encode}(B) + \mathrm{intermediate~state}.\]

      </div>
    </div>
  </main>

  <!-- Footer with basic info -->
  <footer>
    <p>&copy; 2026 Fruit of Preterition. Powered by <a href="https://pages.github.com/">GitHub
        Pages</a>.</p>
  </footer>
</body>

</html>