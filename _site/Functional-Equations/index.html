<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Functional Equations: The Algebra Behind Iteration</title>

  <!-- Tag styling -->
  <style>
    .tags { margin: 0.5em 0 1.5em 0; }
    .tag {
      display: inline-block;
      padding: 2px 8px;
      margin: 2px;
      background: rgba(255,255,255,0.08);
      border-radius: 3px;
      font-size: 0.85em;
      text-decoration: none;
      color: var(--muted);
    }
    .tag:hover { background: rgba(255,255,255,0.15); color: var(--accent); }
  </style>

  <!-- MathJax configuration for LaTeX rendering -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        ignoreHtmlClass: "tex2jax_ignore",
        processHtmlClass: "tex2jax_process"
      },
      svg: {
        fontCache: 'global'
      },
      startup: {
        pageReady: function () {
          return MathJax.startup.defaultPageReady().then(function () {
            console.log('MathJax is loaded and ready');
          });
        }
      }
    };
  </script>
  <script type="text/javascript" async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-svg.js"></script>

  <!-- Site stylesheet -->
  <link rel="stylesheet" href="/crw-blog/assets/css/style.css">
</head>

<body>
  <!-- Header and site title -->
  <header>
    <h1>Fruit of Preterition</h1>
    <nav>
      <ul>
        <li><a href="/crw-blog/">Home</a></li>
        <li><a href="/crw-blog/seeds/">Seeds <span class="wip-badge">WIP</span></a></li>
        <li><a href="/crw-blog/tags/">Tags <span class="wip-badge">WIP</span></a></li>
        <li><a href="/crw-blog/reading/">Reading <span class="wip-badge">WIP</span></a></li>
        <li><a href="/crw-blog/nulla-die/">Nulla Die</a></li>
      </ul>
    </nav>
  </header>

  <!-- Main content of the page -->
  <main>
    <div class="site-container">
      <div class="content">
        
        <div class="tags">
          
          <a href="/crw-blog/tags/#seed" class="tag">seed</a>
          
          <a href="/crw-blog/tags/#math" class="tag">math</a>
          
        </div>
        
        <h1 id="functional-equations-the-algebra-behind-iteration">Functional Equations: The Algebra Behind Iteration</h1>

<h2 id="the-thesis">The Thesis</h2>

<p>Here’s a claim: many things in mathematics and physics that seem gratuitously complicated are secretly about <strong>functional equations</strong>. And functional equations are, indeed, genuinely complicated—not because mathematicians made them hard, but because they touch something fundamental about composition and iteration.</p>

<p>What’s a functional equation? It’s an equation where the unknown is a <em>function</em> rather than a number. You’re probably used to solving $x^2 - 5x + 6 = 0$ for $x$. A functional equation asks you to solve something like:</p>

\[f(x + y) = f(x) + f(y)\]

<p>for the function $f$. (Answer: $f(x) = cx$ for any constant $c$, assuming continuity. Without continuity? Monsters.)</p>

<p>The functional equations I want to talk about today are specifically about <strong>iteration and composition</strong>. When you compose a function with itself—$f \circ f$, $f \circ f \circ f$, etc.—strange algebraic structures emerge. The simplest questions become hard:</p>

<ul>
  <li>Given $g$, find $f$ such that $f \circ f = g$ (the “functional square root”)</li>
  <li>Describe all functions that commute: $f \circ g = g \circ f$</li>
  <li>Interpolate between integer iterates: what’s $f^{1/2}$ or $f^\pi$?</li>
</ul>

<p>These aren’t contrived puzzles. They show up in:</p>
<ul>
  <li>Dynamical systems and chaos</li>
  <li>Renormalization group flows (where the beta function is an infinitesimal generator)</li>
  <li>Formal power series and combinatorics</li>
  <li>Fractional calculus and continuous iteration</li>
</ul>

<p>The punchline is that solving these equations requires thinking about <strong>eigenvalues and conjugacy</strong>—the same ideas from linear algebra, but for nonlinear maps.</p>

<h2 id="the-schröder-equation-linearizing-iteration">The Schröder Equation: Linearizing Iteration</h2>

<p>Let’s start with the most important functional equation for dynamics. Suppose you have a function $f(x)$ and you want to understand its iterates $f^n = f \circ f \circ \cdots \circ f$ ($n$ times).</p>

<p><strong>Schröder’s equation</strong> asks: can we find a change of coordinates $\phi$ such that iteration becomes <em>linear</em>?</p>

\[\boxed{\phi(f(x)) = \lambda \, \phi(x)}\]

<p>If you can solve this, then:</p>

\[\phi(f^n(x)) = \lambda^n \phi(x)\]

<p>So in the $\phi$-coordinates, iteration is just multiplication by $\lambda^n$. To compute $f^n(x)$, you’d do:</p>

\[f^n(x) = \phi^{-1}(\lambda^n \phi(x))\]

<p>This is <strong>exactly</strong> like diagonalizing a matrix. If $A = PDP^{-1}$ where $D$ is diagonal, then $A^n = PD^n P^{-1}$. Schröder’s equation does this for nonlinear maps.</p>

<h3 id="when-does-it-work">When Does It Work?</h3>

<p>Near a fixed point $f(x_0) = x_0$, if $\lambda = f’(x_0) \neq 0, 1$, you can (generically) solve Schröder’s equation locally. The $\lambda$ in the equation is exactly this multiplier.</p>

<p><strong>Example:</strong> Take $f(x) = 2x + x^2$ near $x = 0$. We have $f(0) = 0$ and $f’(0) = 2$. Schröder’s equation becomes $\phi(2x + x^2) = 2\phi(x)$.</p>

<p>You can solve this by expanding $\phi(x) = x + a_2 x^2 + a_3 x^3 + \cdots$ and matching coefficients:</p>

\[\phi(2x + x^2) = 2x + x^2 + a_2(2x + x^2)^2 + \cdots = 2\phi(x) = 2x + 2a_2 x^2 + \cdots\]

<p>Matching $x^2$: $1 + 4a_2 = 2a_2$, so $a_2 = -1/2$.</p>

<p>And so on. The series converges, and you get a genuine conjugacy to linear dynamics.</p>

<h3 id="the-hard-part">The Hard Part</h3>

<p>What if $\lambda = 1$? Then Schröder’s equation has no nontrivial solutions (you’d need $\phi(f(x)) = \phi(x)$, meaning $\phi$ is constant on orbits). This is where <strong>Abel’s equation</strong> comes in.</p>

<h2 id="abels-equation-when-schröder-fails">Abel’s Equation: When Schröder Fails</h2>

<p>When $f’(x_0) = 1$ (a “parabolic” fixed point), we need a different approach. <strong>Abel’s equation</strong> asks for a function $\alpha$ such that:</p>

\[\boxed{\alpha(f(x)) = \alpha(x) + 1}\]

<p>This turns iteration into <em>addition</em>:</p>

\[\alpha(f^n(x)) = \alpha(x) + n\]

<p>So $f^n(x) = \alpha^{-1}(\alpha(x) + n)$.</p>

<p><strong>Connection:</strong> If you have a Schröder function $\phi$ with $\phi(f(x)) = \lambda \phi(x)$, then $\alpha(x) = \log_\lambda \phi(x)$ solves Abel’s equation. So Abel and Schröder are related by a logarithm—exactly like the relationship between exponential growth ($y’ = \lambda y$) and linear growth ($y’ = c$).</p>

<h3 id="example-the-parabolic-case">Example: The Parabolic Case</h3>

<p>Consider $f(x) = x + x^2$ near $x = 0$. Here $f(0) = 0$, $f’(0) = 1$. Schröder doesn’t work.</p>

<p>Abel’s equation: $\alpha(x + x^2) = \alpha(x) + 1$.</p>

<p>For small $x$, iteration looks like: $x_1 = x + x^2 \approx x$, $x_2 \approx x + 2x^2$, … The orbit moves slowly away from 0, with $x_n \approx x/(1 - nx)$ for small $x$.</p>

<p>The Abel function turns out to be $\alpha(x) = -1/x + O(\log x)$ as $x \to 0^+$. Check: $\alpha(f(x)) = -1/(x + x^2) = -1/x \cdot 1/(1+x) \approx -1/x + 1 = \alpha(x) + 1$.</p>

<h2 id="the-jabotinsky-matrix-formal-power-series-sorcery">The Jabotinsky Matrix: Formal Power Series Sorcery</h2>

<p>Now here’s where it gets algebraically interesting. Suppose we work with formal power series:</p>

\[f(x) = \sum_{n=1}^\infty a_n x^n, \quad a_1 \neq 0\]

<p>Composition of such series is a group operation (assuming $a_1 \neq 0$ so there’s a compositional inverse). Can we understand this group algebraically?</p>

<p><strong>Jabotinsky’s idea:</strong> represent composition as matrix multiplication.</p>

<p>If $f(x) = \sum a_n x^n$ and $g(x) = \sum b_n x^n$, then $(f \circ g)(x) = \sum c_n x^n$ where the $c_n$ depend on $a_i$ and $b_j$ in a complicated but polynomial way.</p>

<p>Define the <strong>Jabotinsky matrix</strong> $J_f$ whose $(i,j)$ entry encodes how $f$ acts on the $j$-th coefficient when you compose:</p>

\[(J_f)_{ij} = [x^i] f(x)^j / j!\]

<p>where $[x^i]$ means “coefficient of $x^i$”. Then:</p>

\[J_{f \circ g} = J_f \cdot J_g\]

<p>Composition becomes matrix multiplication! The group of formal diffeomorphisms embeds into infinite matrices.</p>

<h3 id="the-lie-algebra">The Lie Algebra</h3>

<p>Matrix groups have Lie algebras. What’s the Lie algebra of the composition group?</p>

<p>A one-parameter subgroup $f_t$ satisfies $f_{s+t} = f_s \circ f_t$. Differentiating at $t = 0$:</p>

\[\frac{d}{dt}\bigg|_{t=0} f_t(x) = v(x)\]

<p>This vector field $v$ generates the flow $f_t$. The relationship is:</p>

\[\frac{\partial f_t}{\partial t} = v(f_t(x))\]

<p>or equivalently, the Abel function $\alpha$ for $f_t$ satisfies $v(x) \cdot \alpha’(x) = 1$.</p>

<p><strong>The Lie algebra elements are vector fields!</strong> Composition of diffeomorphisms corresponds to exponentiating vector fields.</p>

<h2 id="connection-to-physics-the-beta-function">Connection to Physics: The Beta Function</h2>

<p>Consider a renormalization group flow. You have a coupling constant $g$ that runs with scale $\mu$. Under a scale transformation $\mu \to e^t \mu$, the coupling transforms:</p>

\[g(e^t \mu) = f_t(g(\mu))\]

<p>where $f_t$ is some flow map. By the group property, $f_{s+t} = f_s \circ f_t$.</p>

<p>The <strong>beta function</strong> is exactly the generator of this one-parameter family:</p>

\[\boxed{\beta(g) = \frac{d}{dt}\bigg|_{t=0} f_t(g) = \mu \frac{dg}{d\mu}}\]

<p>This is the infinitesimal version of “how does the coupling change under scale transformations.”</p>

<p><strong>The beta function is the vector field that generates functional composition of the RG flow.</strong></p>

<p>The functional equation perspective clarifies several things:</p>
<ol>
  <li><strong>Fixed points</strong>: $\beta(g_<em>) = 0$ means $g_</em>$ is a fixed point of the flow. The eigenvalue of $d\beta/dg$ at $g_*$ determines stability.</li>
  <li><strong>Scheme dependence</strong>: Different renormalization schemes give different $f_t$, hence different $\beta$. But fixed points and their stability (eigenvalues) are conjugacy invariants.</li>
  <li><strong>Why it’s hard</strong>: The RG flow is a dynamical system in infinite dimensions (the space of all couplings). Functional equations in this setting are genuinely complicated.</li>
</ol>

<h2 id="fractional-iteration-can-you-take-the-square-root-of-a-function">Fractional Iteration: Can You Take the Square Root of a Function?</h2>

<p>Here’s a fun question: given $f(x)$, can you find $g(x)$ such that $g \circ g = f$? This is the “functional square root” or “half-iterate” of $f$.</p>

<p>Schröder’s equation gives the answer (when it works):</p>

\[g(x) = \phi^{-1}(\sqrt{\lambda} \, \phi(x))\]

<p>More generally, fractional iterates:</p>

\[f^t(x) = \phi^{-1}(\lambda^t \phi(x))\]

<p>This interpolates between $f^0 = \text{id}$ and $f^1 = f$ smoothly.</p>

<p><strong>The catch:</strong> $\sqrt{\lambda}$ has two values, so there are (at least) two square roots. And if $\lambda &lt; 0$, you need complex Schröder functions. And at parabolic points, it’s worse.</p>

<h3 id="a-classic-hard-problem">A Classic Hard Problem</h3>

<p>What’s the half-iterate of $f(x) = e^x$?</p>

<p>This is surprisingly hard. There’s no fixed point with nice multiplier (the only real fixed point is $x \to -\infty$ as an essential singularity). People have constructed various “solutions” using different analytic continuations, but there’s no canonical answer.</p>

<p>The difficulty of this problem—finding $g$ with $g(g(x)) = e^x$—illustrates how these innocent-looking equations contain real complexity.</p>

<h2 id="julias-equation-and-commutators">Julia’s Equation and Commutators</h2>

<p>One more for the road. <strong>Julia’s equation</strong> asks: when do two functions commute under composition?</p>

\[f \circ g = g \circ f\]

<p>For formal power series near a fixed point, this is highly constrained. If $f$ and $g$ both fix $0$ with multipliers $\lambda$ and $\mu$:</p>

<ul>
  <li>
    <p>If $\lambda$ and $\mu$ are multiplicatively independent (no relation $\lambda^m = \mu^n$ for integers $m, n$), then $f$ and $g$ commute only if they’re both iterates of some common function $h$.</p>
  </li>
  <li>
    <p>If $\lambda$ and $\mu$ are roots of unity, the analysis is more delicate.</p>
  </li>
</ul>

<p>This connects to <strong>centralizers</strong> in the diffeomorphism group: the centralizer of $f$ (functions commuting with $f$) is generically just the iterates of $f$ itself.</p>

<h2 id="summary-the-functional-equation-zoo">Summary: The Functional Equation Zoo</h2>

<table>
  <thead>
    <tr>
      <th>Equation</th>
      <th>Form</th>
      <th>Purpose</th>
      <th>When it works</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Schröder</td>
      <td>$\phi(f(x)) = \lambda \phi(x)$</td>
      <td>Linearize iteration</td>
      <td>$f’(x_0) \neq 0, 1$</td>
    </tr>
    <tr>
      <td>Abel</td>
      <td>$\alpha(f(x)) = \alpha(x) + 1$</td>
      <td>Turn iteration to addition</td>
      <td>$f’(x_0) = 1$</td>
    </tr>
    <tr>
      <td>Böttcher</td>
      <td>$\phi(f(x)) = \phi(x)^d$</td>
      <td>Handle critical points</td>
      <td>$f(x) = x^d + \cdots$</td>
    </tr>
    <tr>
      <td>Julia</td>
      <td>$f(g(x)) = g(f(x))$</td>
      <td>Characterize commutants</td>
      <td>Various</td>
    </tr>
  </tbody>
</table>

<p>The Jabotinsky matrix packages all of this into linear algebra over infinite dimensions.</p>

<p>And whenever something in dynamics or physics seems needlessly complicated, there’s a decent chance it’s secretly a functional equation in disguise. Now you know what to look for.</p>

<hr />

<p><em>Further reading: the classic reference is Kuczma, Choczewski &amp; Ger’s “Iterative Functional Equations”. For the RG connection, anything by Jean Zinn-Justin. For Jabotinsky matrices and formal power series, look up the work of Eri Jabotinsky or more modern expositions in Berg &amp; Duistermaat.</em></p>

      </div>
    </div>
  </main>

  <!-- Footer with basic info -->
  <footer>
    <p>&copy; 2026 Fruit of Preterition. Powered by <a href="https://pages.github.com/">GitHub
        Pages</a>.</p>
  </footer>
</body>

</html>